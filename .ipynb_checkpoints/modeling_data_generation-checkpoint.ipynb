{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r __importData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_extraction import AirlineFeature, GroupFeature, OrderFeature, \\\n",
    "                               MixingFeature, MeanEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_feature(table, key_words):\n",
    "    features = []\n",
    "    \"\"\"key_words = [\"year\", \"month\", \"DoW\", \"DoM\", \"sub_line\", \"area\", \"unit\",\n",
    "                 \"source_1\", \"source_2\", \"hour\", \"airport\", \"consistent\",\n",
    "                 \"quarter\", \"part_of_day\"]\"\"\"\n",
    "    #key_words = [\"part_of_day\", \"airport\"]\n",
    "    #key_words = [\"year\", \"DoW\", \"sub_line\", \"area\", \"unit\",\n",
    "    #             \"source_1\", \"source_2\", \"hour\", \"airport\", \"consistent\"]\n",
    "    #key_words = [\"year\", \"DoW\", \"source_1\", \"source_2\", \"airport\", \"consistent\"]\n",
    "    for col in table.columns:\n",
    "        for keyword in key_words:\n",
    "            if keyword in col and col not in features:\n",
    "                features.append(col)\n",
    "    return features\n",
    "\n",
    "\n",
    "def feature_transform(input_table, features):\n",
    "    table = input_table.copy()\n",
    "    for col in features:\n",
    "        feature = pd.get_dummies(table[col])\n",
    "        new_name = []\n",
    "        for feature_col in feature.columns:\n",
    "            if \"part\" in col:\n",
    "                new_name.append(col + \"_\" + str(feature_col))\n",
    "            else:\n",
    "                new_name.append(col + \"_\" + str(int(feature_col)))\n",
    "        feature.columns = new_name\n",
    "        table = pd.concat([table, feature], axis=1)\n",
    "    table.drop(columns=features, inplace=True)\n",
    "    return table\n",
    "\n",
    "def feature_transform2(input_table, features):\n",
    "    table = input_table.copy()\n",
    "    for col in features:\n",
    "        le = LabelEncoder()\n",
    "        table[col] = le.fit_transform(table[col].astype(str))\n",
    "    return table\n",
    "\n",
    "def feature_transform3(input_table, features):\n",
    "    table = input_table.copy()\n",
    "    for feature in features:\n",
    "        df = pd.DataFrame(table[feature].value_counts())\n",
    "        new_encode = {}\n",
    "        for i, cate_name in enumerate(df.index.values):\n",
    "            new_encode[cate_name] = df.iloc[i][0]\n",
    "        table[feature] = table[feature].map(new_encode)\n",
    "    return table\n",
    "\n",
    "def impact_coding(data, feature, target='y'):\n",
    "    '''\n",
    "    In this implementation we get the values and the dictionary as two different steps.\n",
    "    This is just because initially we were ignoring the dictionary as a result variable.\n",
    "\n",
    "    In this implementation the KFolds use shuffling. If you want reproducibility the cv\n",
    "    could be moved to a parameter.\n",
    "    '''\n",
    "    n_folds = 5\n",
    "    n_inner_folds = 2\n",
    "    impact_coded = pd.Series()\n",
    "\n",
    "    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    oof_mean_cv = pd.DataFrame()\n",
    "    split = 0\n",
    "    for infold, oof in kf.split(data[feature]):\n",
    "            impact_coded_cv = pd.Series()\n",
    "            kf_inner = KFold(n_splits=n_inner_folds, shuffle=True)\n",
    "            inner_split = 0\n",
    "            inner_oof_mean_cv = pd.DataFrame()\n",
    "            oof_default_inner_mean = data.iloc[infold][target].mean()\n",
    "            for infold_inner, oof_inner in kf_inner.split(data.iloc[infold]):\n",
    "                # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)\n",
    "                oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n",
    "                impact_coded_cv = impact_coded_cv.append(data.iloc[infold].apply(\n",
    "                            lambda x: oof_mean[x[feature]]\n",
    "                                      if x[feature] in oof_mean.index\n",
    "                                      else oof_default_inner_mean\n",
    "                            , axis=1))\n",
    "\n",
    "                # Also populate mapping (this has all group -> mean for all inner CV folds)\n",
    "                inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n",
    "                inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n",
    "                inner_split += 1\n",
    "\n",
    "            # Also populate mapping\n",
    "            oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n",
    "            oof_mean_cv.fillna(value=oof_default_mean, inplace=True)\n",
    "            split += 1\n",
    "\n",
    "            impact_coded = impact_coded.append(data.iloc[oof].apply(\n",
    "                            lambda x: inner_oof_mean_cv.loc[x[feature]].mean()\n",
    "                                      if x[feature] in inner_oof_mean_cv.index\n",
    "                                      else oof_default_mean\n",
    "                            , axis=1))\n",
    "\n",
    "    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n",
    "\n",
    "def train_validation_split(train_set, train_y):\n",
    "    train_data = train_set.iloc[0:146620]\n",
    "    train_target = train_y[0:146620]\n",
    "    validation_data = train_set.iloc[146620:]\n",
    "    validation_target = train_y[146620:]\n",
    "    return (train_data, validation_data, train_target, validation_target)\n",
    "\n",
    "\n",
    "def mean_encoding(train, test, enc_feature):\n",
    "    x_train = train.copy()\n",
    "    x_test = test.copy()\n",
    "    impact_coding_map = {}\n",
    "    for f in enc_feature:\n",
    "        print(\"Impact coding for {}\".format(f))\n",
    "        x_train[\"impact_encoded_{}\".format(f)], impact_coding_mapping, default_coding = impact_coding(x_train, f, \"deal_or_not\")\n",
    "        impact_coding_map[f] = (impact_coding_mapping, default_coding)\n",
    "        mapping, default_mean = impact_coding_map[f]\n",
    "        x_test[\"impact_encoded_{}\".format(f)] = x_test.apply(lambda x: mapping[x[f]]\n",
    "                                                                         if x[f] in mapping\n",
    "                                                                         else default_mean\n",
    "                                                               , axis=1)\n",
    "    return (x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import group_table\n",
      "import airline_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2850: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import order_table\n",
      "import train_set\n",
      "import test_set\n"
     ]
    }
   ],
   "source": [
    "__importData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_feature = []\n",
    "group_table = GroupFeature.group_feature(group_table, drop_feature)\n",
    "order_table = OrderFeature.order_feature(order_table, drop_feature)\n",
    "airline_table = AirlineFeature.airline_feature(airline_table, drop_feature)\n",
    "main_table = pd.merge(order_table, group_table, on=\"group_id\", how=\"left\")\n",
    "main_table = pd.merge(main_table, airline_table, on=\"group_id\", how=\"left\")\n",
    "main_table = MixingFeature.mixing_feature(main_table, drop_feature)\n",
    "main_table.drop(columns=drop_feature, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set2 = pd.merge(train_set, main_table, on=\"order_id\", how=\"left\")\n",
    "train_y = train_set2[\"deal_or_not\"]\n",
    "x_train, x_val, y_train, y_val = train_validation_split(train_set2, train_y)\n",
    "test_set2 = pd.merge(test_set, main_table, on=\"order_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_csv(\"dataset/train_val/training_set.csv\", encoding=\"utf-8\", index=False)\n",
    "x_val.to_csv(\"dataset/train_val/validation_set.csv\", encoding=\"utf-8\", index=False)\n",
    "test_set2.to_csv(\"dataset/train_val/testing_set.csv\", encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
